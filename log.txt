Test 1: Accuracy= 0.9833 ; Val_Accuracy= 0.7525 ; Test Accuracy= 72.75
    1 Relu
    2 Relu
    1 Relu
    4 Relu
    2 Relu
    1 Relu
    10* softmax

Test 2: Accuracy= 0.9833 ; Val_Accuracy= 79.25 ; Test Accuracy= 79.0
relevant_dimensions= len(relevant_data[0])
model.add(Dense(relevant_dimensions//2, activation='relu', input_dim=relevant_dimensions)) # This defines the dimensions of the input dimension and 1st hidden layer
model.add(Dropout(0.5)) # We added drop out as half to reduce the underfitting.
model.add(Dense(relevant_dimensions//19, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

Test 3: Accuracy= 0.9667 ; Val_Accuracy= 79.5 ; Test Accuracy= 79.0
relevant_dimensions= len(relevant_data[0])
model.add(Dense(relevant_dimensions//2, activation='relu', input_dim=relevant_dimensions, kernel_initializer='he_uniform', use_bias=True, )) # This defines the dimensions of the input dimension and 1st hidden layer
model.add(Dropout(0.5)) # We added drop out as half to reduce the underfitting.
model.add(Dense(relevant_dimensions//19, activation='relu', kernel_initializer='he_uniform', use_bias=False))
model.add(Dropout(0.5)) # We added drop out as half to reduce the underfitting.
model.add(Dense(10, activation='softmax'))

Test 3: Accuracy= 0.9767 ; Val_Accuracy= 80.0 ; Test Accuracy= 80.5
relevant_dimensions= len(relevant_data[0])
model.add(Dense(relevant_dimensions//2, activation='relu', input_dim=relevant_dimensions, kernel_initializer='he_uniform')) # This defines the dimensions of the input dimension and 1st hidden layer
model.add(Dropout(0.5)) # We added drop out as half to reduce the underfitting.
model.add(Dense(relevant_dimensions//19, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.5)) # We added drop out as half to reduce the underfitting.
model.add(Dense(10, activation='softmax'))

# TODO: Justify why we picked these specific optimizer, loss and metric parameters
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.004, amsgrad=True), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

model.fit(x_train, y_train,  epochs=90,  batch_size=10, shuffle=False)